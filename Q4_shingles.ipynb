{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IamAgranshRastogi/Natural_Language_Processing/blob/main/Q4_shingles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JFu4VB0UPln",
        "outputId": "4eab66b3-55c0-4ccc-bfe2-43bd8a6c80c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For k= 1 shingles \n",
            " [('I',), ('am',), ('Sam',), ('.',), ('Sam',), ('I',), ('am',), ('.',), ('I',), ('do',), ('not',), ('like',), ('green',), ('eggs',), ('and',), ('ham',), ('.',), ('I',), ('do',), ('not',), ('like',), ('them',), (',',), ('Sam',), ('I',), ('am',), ('.',)]\n",
            "\n",
            "\n",
            "For k= 2 shingles \n",
            " [('I', 'am'), ('am', 'Sam'), ('Sam', '.'), ('Sam', 'I'), ('I', 'am'), ('am', '.'), ('I', 'do'), ('do', 'not'), ('not', 'like'), ('like', 'green'), ('green', 'eggs'), ('eggs', 'and'), ('and', 'ham'), ('ham', '.'), ('I', 'do'), ('do', 'not'), ('not', 'like'), ('like', 'them'), ('them', ','), (',', 'Sam'), ('Sam', 'I'), ('I', 'am'), ('am', '.')]\n",
            "\n",
            "\n",
            "For k-3 character Shingle \n",
            " [('I', ' ', 'a'), (' ', 'a', 'm'), ('a', 'm', ' '), ('m', ' ', 'S'), (' ', 'S', 'a'), ('S', 'a', 'm'), ('a', 'm', '.'), ('S', 'a', 'm'), ('a', 'm', ' '), ('m', ' ', 'I'), (' ', 'I', ' '), ('I', ' ', 'a'), (' ', 'a', 'm'), ('a', 'm', '.')]\n",
            "\n",
            "\n",
            "For k-4 character Shingle \n",
            " [('I', ' ', 'd'), (' ', 'd', 'o'), ('d', 'o', ' '), ('o', ' ', 'n'), (' ', 'n', 'o'), ('n', 'o', 't'), ('o', 't', ' '), ('t', ' ', 'l'), (' ', 'l', 'i'), ('l', 'i', 'k'), ('i', 'k', 'e'), ('k', 'e', ' '), ('e', ' ', 'g'), (' ', 'g', 'r'), ('g', 'r', 'e'), ('r', 'e', 'e'), ('e', 'e', 'n'), ('e', 'n', ' '), ('n', ' ', 'e'), (' ', 'e', 'g'), ('e', 'g', 'g'), ('g', 'g', 's'), ('g', 's', ' '), ('s', ' ', 'a'), (' ', 'a', 'n'), ('a', 'n', 'd'), ('n', 'd', ' '), ('d', ' ', 'h'), (' ', 'h', 'a'), ('h', 'a', 'm'), ('a', 'm', '.'), ('I', ' ', 'd'), (' ', 'd', 'o'), ('d', 'o', ' '), ('o', ' ', 'n'), (' ', 'n', 'o'), ('n', 'o', 't'), ('o', 't', ' '), ('t', ' ', 'l'), (' ', 'l', 'i'), ('l', 'i', 'k'), ('i', 'k', 'e'), ('k', 'e', ' '), ('e', ' ', 't'), (' ', 't', 'h'), ('t', 'h', 'e'), ('h', 'e', 'm'), ('e', 'm', ','), ('m', ',', ' '), (',', ' ', 'S'), (' ', 'S', 'a'), ('S', 'a', 'm'), ('a', 'm', ' '), ('m', ' ', 'I'), (' ', 'I', ' '), ('I', ' ', 'a'), (' ', 'a', 'm'), ('a', 'm', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import nltk.corpus\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "d1= \"I am Sam.\"\n",
        "d2 =\"Sam I am.\"\n",
        "d3= \"I do not like green eggs and ham.\"\n",
        "d4=\"I do not like them, Sam I am.\"\n",
        "\n",
        "#Tokenizing the Strings\n",
        "\n",
        "d1_tokens= word_tokenize(d1)\n",
        "\n",
        "d2_tokens=word_tokenize(d2)\n",
        "\n",
        "d3_tokens=word_tokenize(d3)\n",
        "d4_tokens=word_tokenize(d4)\n",
        "\n",
        "#Making shingles for k=1 and k=2\n",
        "\n",
        "for i in range(2):\n",
        "  d1_shingle1=list(nltk.ngrams (d1_tokens,i+1))\n",
        "  d2_shingle1=list(nltk.ngrams (d2_tokens, i+1))\n",
        "  d3_shingle1=list(nltk.ngrams (d3_tokens, i+1))\n",
        "  d4_shingle1=list(nltk.ngrams (d4_tokens, i+1))\n",
        "\n",
        "  print(\"For k=\", i+1, \"shingles \\n\", d1_shingle1 + d2_shingle1 + d3_shingle1 + d4_shingle1)\n",
        "\n",
        "  print(\"\\n\")\n",
        "\n",
        "#Character shingles k-3 and k-4\n",
        "\n",
        "for i in range(2):\n",
        "\n",
        "   dl_shingle2=list (nltk.ngrams(d1,1+2))\n",
        "   d2_shingle2=list (nltk.ngrams (d2,1+2))\n",
        "   d3_shingle2=list (nltk.ngrams (d3,1+2))\n",
        "   d4_shingle2=list (nltk.ngrams (d4,1+2))\n",
        "\n",
        "\n",
        "\n",
        "print(\"For k-3 character Shingle \\n\",dl_shingle2 + d2_shingle2)\n",
        "print(\"\\n\")\n",
        "print(\"For k-4 character Shingle \\n\",d3_shingle2 + d4_shingle2)\n",
        "\n",
        "#We can exclude all the whitespaces by using string.replace(\"10."
      ]
    }
  ]
}