{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPQywzUub3bw9kbumvclAeu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Tokenization**\n","* Tokenization refers to break down the text into smaller units.\n","* It entails splitting paragraphs into sentences and sentences into words.\n","* It is one of the initial steps of any NLP pipeline.\n","* Let us have a look at the two major kinds of tokenization that NLTK provides:"],"metadata":{"id":"La3xApkZP-JI"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X1gsWg1a8J3-","executionInfo":{"status":"ok","timestamp":1726774578973,"user_tz":-330,"elapsed":8443,"user":{"displayName":"Agransh Rastogi","userId":"10194062709337204642"}},"outputId":"1b63b0d3-d904-41b2-d2c1-ba471714bf88"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"]}],"source":["!pip install nltk"]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gP4dBeshODxy","executionInfo":{"status":"ok","timestamp":1726774582620,"user_tz":-330,"elapsed":3658,"user":{"displayName":"Agransh Rastogi","userId":"10194062709337204642"}},"outputId":"36317ac6-56f3-480f-8039-093a45e08d86"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["corpus = \"\"\"  Tokenization is one of the first step in any NLP pipeline.\n","Tokenization is nothing but splitting the raw text into small chunks of words or sentences, called tokens.\n","\"\"\""],"metadata":{"id":"97jd1SpR8Yoq","executionInfo":{"status":"ok","timestamp":1726774582621,"user_tz":-330,"elapsed":63,"user":{"displayName":"Agransh Rastogi","userId":"10194062709337204642"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["print(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hLvzHgqy9iAb","executionInfo":{"status":"ok","timestamp":1726774582621,"user_tz":-330,"elapsed":62,"user":{"displayName":"Agransh Rastogi","userId":"10194062709337204642"}},"outputId":"6ea1c490-957c-4cb1-8744-308f3d88f2c2"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["  Tokenization is one of the first step in any NLP pipeline.\n","Tokenization is nothing but splitting the raw text into small chunks of words or sentences, called tokens.\n","\n"]}]},{"cell_type":"markdown","source":["**Work Tokenization :**\n","It involves breaking down the text into words."],"metadata":{"id":"4JVCKNhuRJ6l"}},{"cell_type":"code","source":["## Tokenization\n","## Sentence --> Paragraphs\n","from nltk.tokenize import sent_tokenize"],"metadata":{"id":"jzSZzdkI9kHQ","executionInfo":{"status":"ok","timestamp":1726774582622,"user_tz":-330,"elapsed":60,"user":{"displayName":"Agransh Rastogi","userId":"10194062709337204642"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["docs = sent_tokenize(corpus)"],"metadata":{"id":"XXEXm9qU93Ua","executionInfo":{"status":"ok","timestamp":1726774582622,"user_tz":-330,"elapsed":58,"user":{"displayName":"Agransh Rastogi","userId":"10194062709337204642"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["docs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X_vAj6Mg98HD","executionInfo":{"status":"ok","timestamp":1726774582623,"user_tz":-330,"elapsed":57,"user":{"displayName":"Agransh Rastogi","userId":"10194062709337204642"}},"outputId":"28753afb-546f-4836-8413-7bbe0fc27509"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['  Tokenization is one of the first step in any NLP pipeline.',\n"," 'Tokenization is nothing but splitting the raw text into small chunks of words or sentences, called tokens.']"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["type(docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ijqNTRdDMctm","executionInfo":{"status":"ok","timestamp":1726774582623,"user_tz":-330,"elapsed":53,"user":{"displayName":"Agransh Rastogi","userId":"10194062709337204642"}},"outputId":"7399947e-ff5a-49ba-e8f3-1f43c0ca28c8"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["list"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["for sentence in docs:\n","  print(sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JUyNwP4mMcSE","executionInfo":{"status":"ok","timestamp":1726774582623,"user_tz":-330,"elapsed":49,"user":{"displayName":"Agransh Rastogi","userId":"10194062709337204642"}},"outputId":"95d03442-ffde-4f53-f57c-bc7205a81a47"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["  Tokenization is one of the first step in any NLP pipeline.\n","Tokenization is nothing but splitting the raw text into small chunks of words or sentences, called tokens.\n"]}]},{"cell_type":"markdown","source":["**Sentence Tokenization :**\n","It involves breaking down the text into individual sentences."],"metadata":{"id":"hnofpBoHRWLZ"}},{"cell_type":"code","source":["## Tokenization\n","## Paragraph --> Words\n","## Sentence --> Words\n","from nltk.tokenize import word_tokenize"],"metadata":{"id":"KWXOwcg-OKMp","executionInfo":{"status":"ok","timestamp":1726774582623,"user_tz":-330,"elapsed":46,"user":{"displayName":"Agransh Rastogi","userId":"10194062709337204642"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["word_tokenize(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SJ6GxHmbOKJG","executionInfo":{"status":"ok","timestamp":1726774582624,"user_tz":-330,"elapsed":47,"user":{"displayName":"Agransh Rastogi","userId":"10194062709337204642"}},"outputId":"46bbba23-7ca1-442d-9bf9-f22c48706544"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Tokenization',\n"," 'is',\n"," 'one',\n"," 'of',\n"," 'the',\n"," 'first',\n"," 'step',\n"," 'in',\n"," 'any',\n"," 'NLP',\n"," 'pipeline',\n"," '.',\n"," 'Tokenization',\n"," 'is',\n"," 'nothing',\n"," 'but',\n"," 'splitting',\n"," 'the',\n"," 'raw',\n"," 'text',\n"," 'into',\n"," 'small',\n"," 'chunks',\n"," 'of',\n"," 'words',\n"," 'or',\n"," 'sentences',\n"," ',',\n"," 'called',\n"," 'tokens',\n"," '.']"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["for sentence in docs:\n","  print(word_tokenize(sentence))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"11Wf0t0FOKGo","executionInfo":{"status":"ok","timestamp":1726774582624,"user_tz":-330,"elapsed":43,"user":{"displayName":"Agransh Rastogi","userId":"10194062709337204642"}},"outputId":"da29ddc4-3783-4bf4-880b-8582f0f13288"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["['Tokenization', 'is', 'one', 'of', 'the', 'first', 'step', 'in', 'any', 'NLP', 'pipeline', '.']\n","['Tokenization', 'is', 'nothing', 'but', 'splitting', 'the', 'raw', 'text', 'into', 'small', 'chunks', 'of', 'words', 'or', 'sentences', ',', 'called', 'tokens', '.']\n"]}]},{"cell_type":"code","source":["from nltk.tokenize import wordpunct_tokenize"],"metadata":{"id":"BnWbPcBUOKDn","executionInfo":{"status":"ok","timestamp":1726774582624,"user_tz":-330,"elapsed":39,"user":{"displayName":"Agransh Rastogi","userId":"10194062709337204642"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["wordpunct_tokenize(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"11Vh4Go4OKAy","executionInfo":{"status":"ok","timestamp":1726774582624,"user_tz":-330,"elapsed":38,"user":{"displayName":"Agransh Rastogi","userId":"10194062709337204642"}},"outputId":"45544923-af4e-4702-d19d-310fdccf668f"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Tokenization',\n"," 'is',\n"," 'one',\n"," 'of',\n"," 'the',\n"," 'first',\n"," 'step',\n"," 'in',\n"," 'any',\n"," 'NLP',\n"," 'pipeline',\n"," '.',\n"," 'Tokenization',\n"," 'is',\n"," 'nothing',\n"," 'but',\n"," 'splitting',\n"," 'the',\n"," 'raw',\n"," 'text',\n"," 'into',\n"," 'small',\n"," 'chunks',\n"," 'of',\n"," 'words',\n"," 'or',\n"," 'sentences',\n"," ',',\n"," 'called',\n"," 'tokens',\n"," '.']"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["from nltk.tokenize import TreebankWordTokenizer"],"metadata":{"id":"F61nU_EQPeSV","executionInfo":{"status":"ok","timestamp":1726774582625,"user_tz":-330,"elapsed":35,"user":{"displayName":"Agransh Rastogi","userId":"10194062709337204642"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["tokenizer = TreebankWordTokenizer()"],"metadata":{"id":"1bKHyyjDPpEI","executionInfo":{"status":"ok","timestamp":1726774582625,"user_tz":-330,"elapsed":34,"user":{"displayName":"Agransh Rastogi","userId":"10194062709337204642"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["tokenizer.tokenize(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6z3JV9DWPqTp","executionInfo":{"status":"ok","timestamp":1726774582625,"user_tz":-330,"elapsed":34,"user":{"displayName":"Agransh Rastogi","userId":"10194062709337204642"}},"outputId":"c9970821-78df-4338-efd5-1441caa6d202"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Tokenization',\n"," 'is',\n"," 'one',\n"," 'of',\n"," 'the',\n"," 'first',\n"," 'step',\n"," 'in',\n"," 'any',\n"," 'NLP',\n"," 'pipeline.',\n"," 'Tokenization',\n"," 'is',\n"," 'nothing',\n"," 'but',\n"," 'splitting',\n"," 'the',\n"," 'raw',\n"," 'text',\n"," 'into',\n"," 'small',\n"," 'chunks',\n"," 'of',\n"," 'words',\n"," 'or',\n"," 'sentences',\n"," ',',\n"," 'called',\n"," 'tokens',\n"," '.']"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["from nltk.tokenize import TreebankWordTokenizer"],"metadata":{"id":"B6bi40Nzytbl","executionInfo":{"status":"ok","timestamp":1726774794958,"user_tz":-330,"elapsed":510,"user":{"displayName":"Agransh Rastogi","userId":"10194062709337204642"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["tokenizer = TreebankWordTokenizer()"],"metadata":{"id":"17Dq1dN2y867","executionInfo":{"status":"ok","timestamp":1726774820585,"user_tz":-330,"elapsed":776,"user":{"displayName":"Agransh Rastogi","userId":"10194062709337204642"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["tokenizer.tokenize(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PFyhOo-Wy83n","executionInfo":{"status":"ok","timestamp":1726774902661,"user_tz":-330,"elapsed":592,"user":{"displayName":"Agransh Rastogi","userId":"10194062709337204642"}},"outputId":"01e16b4d-49e0-483e-b707-d83fc10f1f9d"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Tokenization',\n"," 'is',\n"," 'one',\n"," 'of',\n"," 'the',\n"," 'first',\n"," 'step',\n"," 'in',\n"," 'any',\n"," 'NLP',\n"," 'pipeline.',\n"," 'Tokenization',\n"," 'is',\n"," 'nothing',\n"," 'but',\n"," 'splitting',\n"," 'the',\n"," 'raw',\n"," 'text',\n"," 'into',\n"," 'small',\n"," 'chunks',\n"," 'of',\n"," 'words',\n"," 'or',\n"," 'sentences',\n"," ',',\n"," 'called',\n"," 'tokens',\n"," '.']"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":[],"metadata":{"id":"Jxt-N0Eszfo6"},"execution_count":null,"outputs":[]}]}